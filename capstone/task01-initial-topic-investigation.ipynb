{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univ. of Illinois Data Mining Project on Coursera\n",
    "## Task 01 - Initial Topic Investigation\n",
    "2018-09-16\n",
    "loganjtravis@gmail.com (Logan Travis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, random\n",
    "import gensim.models as models, gensim.matutils as matutils\n",
    "import nltk\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for repeatability\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "From course page [Week 1 > Task 1 Information > Task 1 Overview](https://www.coursera.org/learn/data-mining-project/supplement/z2jpZ/task-1-overview):\n",
    "\n",
    "> The goal of this task is to explore the Yelp data set to get a sense about what the data look like and their characteristics. You can think about the goal as being to answer questions such as:\n",
    "> \n",
    "> 1. What are the major topics in the reviews? Are they different in the positive and negative reviews? Are they different for different cuisines?\n",
    "> 2. What does the distribution of the number of reviews over other variables (e.g., cuisine, location) look like?\n",
    "> 3. What does the distribution of ratings look like?\n",
    ">\n",
    "> In general, you can address such questions by showing visualization of statistics computed based on the data set or topics extracted from review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Rubric\n",
    "\n",
    "From course page [Week 1 > Task 1 Information > Task 1 Rubric](https://www.coursera.org/learn/data-mining-project/supplement/Xk8lq/task-1-rubric):\n",
    "\n",
    "> You will evaluate your peers' submission for Task 1 using this rubric. While evaluating, consider the following questions:\n",
    "> \n",
    "> * Application of a topic model: Was the description of the topic modeling procedure clear enough such that you can produce the same results?\n",
    "> * Topic visualization: Does the topic visualization effectively display the data?\n",
    "> * Data exploration: Was the description of the two sets of data they selected for comparison clear enough to follow?\n",
    "> * Visualization comparison: Does the visualization component highlight the differences/similarities between the data?\n",
    "> \n",
    "> Note that the examples listed in the \"Excellent\" column are not an exclusive list for each category. You may choose to award 6 points for any effort in your peers' submissions that goes beyond what is required.\n",
    "> \n",
    "> | Criteria | Poor (1 point) | Fair (3 points) | Good (5 points) | Excellent (6 points) |\n",
    "> | --- | --- | --- | --- | --- |\n",
    "> | **Task 1.1: Application of a topic model** | A topic model was either not used or did not generate any topic. | A topic model was used, but the report fails to mention what model was used and/or how it is applied to the data set. | The report clearly explains what topic model was used and how it was applied to the data set. | For example, multiple topic models were used and the report analyzes the differences between them. |\n",
    "> | **Task 1.1: Generated visualization** | The visualization is either absent or useless. | The visualization is present but does not help make clear what topics the people have talked about in the reviews. | The visualization clearly shows and distinguishes what topics people have talked about in the reviews. | For example, multiple visualizations were used and the report analyzes the comparative strengths of each.\n",
    "> | **Task 1.2: Generated sets of topics** | The two subsets are not comparable. | The two subsets are comparable. A topic model was used on the two subsets, but the report fails to mention what model was used and/or how it was applied to the data set. | The two subsets are comparable. The report clearly explains what topic model was used and how it was applied to the two subsets. | For example, multiple interesting subsets were identified and assessed for their usefulness, or multiple topic models were applied to the two subsets with differences between them analyzed.\n",
    "> | **Task 1.2: Visualization of comparison** | The two subsets are visualized in such a way that similarities and differences are not clear. | The two subsets are visualized in such a way to show the similarity of the two subsets, but no attempt was made to show the differences. | The two subsets are visualized in such a way that both similarities and differences are very apparent. | Extra transformation of the data was done to improve visualization, or multiple ways of visualizing the topics were used to provide a very comprehensive comparison.\n",
    "> | **Visualizations: Appropriateness of choice** | The visualization methods are not suitable for the type of data. | The visualization methods are suitable for the type of data, but another way to visualize the data is clearly better. | The visualization methods used are quite suitable for the type of data and made relationships clear. | Furthermore, extra effort was made to make the visualizations beautifully designed and/or usefully interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Set\n",
    "\n",
    "Note: I cleaned and saved a Pandas dataframe (as a GZIPped pickle) from the Yelp reviews dataset in a separate notebook \"task00-yelp-reviews-to-pandas-dataframe.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to data source, work in process (\"WIP\"), and output\n",
    "PATH_SOURCE = \"source/\"\n",
    "PATH_WIP = \"wip/\"\n",
    "PATH_OUTPUT = \"output/\"\n",
    "\n",
    "# Set review file path\n",
    "PATH_SOURCE_YELP_REVIEWS = PATH_SOURCE + \"yelp_academic_dataset_review.pkl.gzip\"\n",
    "PATH_WIP_TOKEN_MATRIX = PATH_WIP + \"task01_token_matrix.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickled dataframe\n",
    "dfYelpReviews = pd.read_pickle(PATH_SOURCE_YELP_REVIEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1125458, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15SdjuK7DmYqUAj6rjGowg</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2007-05-17</td>\n",
       "      <td>5</td>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "      <td>review</td>\n",
       "      <td>Xqd0DzHaiyRqVH3WRG7hzg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF6UnRTtG7tWMcrO2GEoAg</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2010-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>review</td>\n",
       "      <td>H1kH6QZV7Le4zqTRNxoZow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-TsVN230RCkLYKBeLsuz7A</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-02-14</td>\n",
       "      <td>4</td>\n",
       "      <td>Dr. Goldberg has been my doctor for years and ...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dNocEAyUucjT371NNND41Q</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>4</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>KBLW4wJA_fwoWmMhiHRVOA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebcN2aqmNUuYNoyvQErgnA</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-05-15</td>\n",
       "      <td>4</td>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   business_id       date  stars  \\\n",
       "review_id                                                          \n",
       "15SdjuK7DmYqUAj6rjGowg  vcNAWiLM4dR7D2nwwJ7nCA 2007-05-17      5   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  vcNAWiLM4dR7D2nwwJ7nCA 2010-03-22      2   \n",
       "-TsVN230RCkLYKBeLsuz7A  vcNAWiLM4dR7D2nwwJ7nCA 2012-02-14      4   \n",
       "dNocEAyUucjT371NNND41Q  vcNAWiLM4dR7D2nwwJ7nCA 2012-03-02      4   \n",
       "ebcN2aqmNUuYNoyvQErgnA  vcNAWiLM4dR7D2nwwJ7nCA 2012-05-15      4   \n",
       "\n",
       "                                                                     text  \\\n",
       "review_id                                                                   \n",
       "15SdjuK7DmYqUAj6rjGowg  dr. goldberg offers everything i look for in a...   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  Unfortunately, the frustration of being Dr. Go...   \n",
       "-TsVN230RCkLYKBeLsuz7A  Dr. Goldberg has been my doctor for years and ...   \n",
       "dNocEAyUucjT371NNND41Q  Been going to Dr. Goldberg for over 10 years. ...   \n",
       "ebcN2aqmNUuYNoyvQErgnA  Got a letter in the mail last week that said D...   \n",
       "\n",
       "                          type                 user_id  votes_cool  \\\n",
       "review_id                                                            \n",
       "15SdjuK7DmYqUAj6rjGowg  review  Xqd0DzHaiyRqVH3WRG7hzg           1   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  review  H1kH6QZV7Le4zqTRNxoZow           0   \n",
       "-TsVN230RCkLYKBeLsuz7A  review  zvJCcrpm2yOZrxKffwGQLA           1   \n",
       "dNocEAyUucjT371NNND41Q  review  KBLW4wJA_fwoWmMhiHRVOA           0   \n",
       "ebcN2aqmNUuYNoyvQErgnA  review  zvJCcrpm2yOZrxKffwGQLA           1   \n",
       "\n",
       "                        votes_funny  votes_useful  \n",
       "review_id                                          \n",
       "15SdjuK7DmYqUAj6rjGowg            0             2  \n",
       "RF6UnRTtG7tWMcrO2GEoAg            0             2  \n",
       "-TsVN230RCkLYKBeLsuz7A            0             1  \n",
       "dNocEAyUucjT371NNND41Q            0             0  \n",
       "ebcN2aqmNUuYNoyvQErgnA            0             2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataframe shape and head\n",
    "print(f\"Shape: {dfYelpReviews.shape}\")\n",
    "dfYelpReviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to load token matrix from file if found; set this to False when changing\n",
    "# other parameters\n",
    "load_token_matrix_from_file = True\n",
    "\n",
    "# Set token limit\n",
    "max_features = 100000\n",
    "\n",
    "# Set document frequency ceiling; topic analysis will ignore words found in more documents\n",
    "max_df = 0.5\n",
    "\n",
    "# Set document frequency floor; topic analysis will ignore words found in fewer document\n",
    "min_df = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tokenizer\n",
    "\n",
    "The `TfidVectorizer` class has a default pre-processor and tokenizer. While the pre-processing steps meet my needs (i.e., puncuation removal and setting lower-case) the tokenizer does not lemmatize nor stem words. Those two additional steps should produce more stable topics. I therefore create my own tokenizer.\n",
    "\n",
    "Note: I create `MyTokenizer` is a class to internalize instantiation of NLK's `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"String tokenizer utilizing lemmatizing and stemming.\"\"\"\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        \"\"\"Return tokens from a string.\"\"\"\n",
    "        return [self.wnl.lemmatize(token) for token in nltk.word_tokenize(document)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized TF-IDF\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer limiting \n",
    "vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, \\\n",
    "                            stop_words=\"english\", use_idf=True, tokenizer=MyTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working dataframe to a 10% sample of the full data set; REPLACE\n",
    "# WITH THE FULL DATA SET LATER\n",
    "# df = dfYelpReviews.sample(frac=0.1)\n",
    "\n",
    "# Set working dataframe to full data set\n",
    "df = dfYelpReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing documents to build token matrix...\n",
      "CPU times: user 3min 37s, sys: 164 ms, total: 3min 37s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load token matrix from file if found and flag set to permit; otherwise vectorize documents\n",
    "tokenMatrix = None\n",
    "if(load_token_matrix_from_file and os.path.isfile(PATH_WIP_TOKEN_MATRIX)):\n",
    "    print(f\"Loading token matrix from file \\\"{PATH_WIP_TOKEN_MATRIX}\\\"...\")\n",
    "    tokenMatrix = load_npz(PATH_WIP_TOKEN_MATRIX)\n",
    "else:\n",
    "    print(\"Vectorizing documents to build token matrix...\")\n",
    "    tokenMatrix = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save token matrix to file\n",
    "save_npz(PATH_WIP_TOKEN_MATRIX, tokenMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 897 tokens in 112,546 documents\n"
     ]
    }
   ],
   "source": [
    "# Print token matrix shape\n",
    "print(\"Found {0[1]:,} tokens in {0[0]:,} documents\".format(tokenMatrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Topics Using LDA\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Set number of words to display for each topic\n",
    "num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GenSim corpus from token vectors\n",
    "corpus = matutils.Sparse2Corpus(tokenMatrix, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for token_id: token\n",
    "id2word = dict([(i, t) for i, t in enumerate(vectorizer.get_feature_names())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.2 s, sys: 20 ms, total: 56.2 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find topics using LDA\n",
    "lda = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"n\\'t\" + 0.010*\"did\" + 0.010*\"service\" + 0.010*\"u\" + 0.010*\"...\" + 0.009*\"time\" + 0.009*\"?\" + 0.009*\"minute\" + 0.008*\"food\" + 0.008*\"\\'\\'\"'),\n",
       " (1,\n",
       "  '0.013*\"\\'s\" + 0.011*\"n\\'t\" + 0.010*\"drink\" + 0.010*\"place\" + 0.009*\"bar\" + 0.009*\")\" + 0.009*\"...\" + 0.008*\"(\" + 0.008*\"like\" + 0.008*\"club\"'),\n",
       " (2,\n",
       "  '0.018*\"burger\" + 0.012*\"sandwich\" + 0.011*\"fry\" + 0.010*\"good\" + 0.010*\"\\'s\" + 0.009*\"breakfast\" + 0.009*\"n\\'t\" + 0.008*\"place\" + 0.008*\"...\" + 0.008*\")\"'),\n",
       " (3,\n",
       "  '0.036*\"room\" + 0.026*\"hotel\" + 0.017*\"stay\" + 0.013*\"pool\" + 0.012*\"casino\" + 0.012*\"strip\" + 0.011*\"nice\" + 0.011*\"\\'s\" + 0.011*\"vega\" + 0.010*\"stayed\"'),\n",
       " (4,\n",
       "  '0.030*\"taco\" + 0.020*\"food\" + 0.017*\"salsa\" + 0.016*\"mexican\" + 0.015*\"good\" + 0.013*\"burrito\" + 0.012*\"chip\" + 0.011*\"place\" + 0.010*\"bean\" + 0.010*\"\\'s\"'),\n",
       " (5,\n",
       "  '0.009*\"chicken\" + 0.009*\"good\" + 0.009*\")\" + 0.008*\"(\" + 0.008*\"food\" + 0.008*\"n\\'t\" + 0.008*\"\\'s\" + 0.007*\"dish\" + 0.007*\"salad\" + 0.007*\"ordered\"'),\n",
       " (6,\n",
       "  '0.032*\"great\" + 0.027*\"food\" + 0.022*\"place\" + 0.021*\"service\" + 0.017*\"good\" + 0.016*\"love\" + 0.014*\"friendly\" + 0.014*\"sushi\" + 0.012*\"atmosphere\" + 0.012*\"staff\"'),\n",
       " (7,\n",
       "  '0.022*\"buffet\" + 0.016*\"food\" + 0.012*\"good\" + 0.011*\"steak\" + 0.009*\"vega\" + 0.009*\"\\'s\" + 0.009*\"n\\'t\" + 0.009*\"$\" + 0.009*\"rib\" + 0.008*\"place\"'),\n",
       " (8,\n",
       "  '0.013*\"store\" + 0.010*\"\\'s\" + 0.010*\"great\" + 0.010*\"n\\'t\" + 0.008*\"time\" + 0.008*\"shop\" + 0.007*\"work\" + 0.007*\")\" + 0.007*\"place\" + 0.006*\"did\"'),\n",
       " (9,\n",
       "  '0.051*\"pizza\" + 0.015*\"\\'s\" + 0.013*\"good\" + 0.012*\"wing\" + 0.011*\"...\" + 0.011*\"place\" + 0.010*\"beer\" + 0.010*\"crust\" + 0.009*\"great\" + 0.009*\"dog\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print topics\n",
    "lda.show_topics(num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
