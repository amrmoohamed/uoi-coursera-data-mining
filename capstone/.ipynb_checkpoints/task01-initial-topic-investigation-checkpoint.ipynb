{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univ. of Illinois Data Mining Project on Coursera\n",
    "## Task 01 - Initial Topic Investigation\n",
    "2018-09-16\n",
    "loganjtravis@gmail.com (Logan Travis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/uoi-capstone/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, pickle, random\n",
    "import gensim.models as models, gensim.matutils as matutils\n",
    "import nltk\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for repeatability\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "From course page [Week 1 > Task 1 Information > Task 1 Overview](https://www.coursera.org/learn/data-mining-project/supplement/z2jpZ/task-1-overview):\n",
    "\n",
    "> The goal of this task is to explore the Yelp data set to get a sense about what the data look like and their characteristics. You can think about the goal as being to answer questions such as:\n",
    "> \n",
    "> 1. What are the major topics in the reviews? Are they different in the positive and negative reviews? Are they different for different cuisines?\n",
    "> 2. What does the distribution of the number of reviews over other variables (e.g., cuisine, location) look like?\n",
    "> 3. What does the distribution of ratings look like?\n",
    ">\n",
    "> In general, you can address such questions by showing visualization of statistics computed based on the data set or topics extracted from review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Rubric\n",
    "\n",
    "From course page [Week 1 > Task 1 Information > Task 1 Rubric](https://www.coursera.org/learn/data-mining-project/supplement/Xk8lq/task-1-rubric):\n",
    "\n",
    "> You will evaluate your peers' submission for Task 1 using this rubric. While evaluating, consider the following questions:\n",
    "> \n",
    "> * Application of a topic model: Was the description of the topic modeling procedure clear enough such that you can produce the same results?\n",
    "> * Topic visualization: Does the topic visualization effectively display the data?\n",
    "> * Data exploration: Was the description of the two sets of data they selected for comparison clear enough to follow?\n",
    "> * Visualization comparison: Does the visualization component highlight the differences/similarities between the data?\n",
    "> \n",
    "> Note that the examples listed in the \"Excellent\" column are not an exclusive list for each category. You may choose to award 6 points for any effort in your peers' submissions that goes beyond what is required.\n",
    "> \n",
    "> | Criteria | Poor (1 point) | Fair (3 points) | Good (5 points) | Excellent (6 points) |\n",
    "> | --- | --- | --- | --- | --- |\n",
    "> | **Task 1.1: Application of a topic model** | A topic model was either not used or did not generate any topic. | A topic model was used, but the report fails to mention what model was used and/or how it is applied to the data set. | The report clearly explains what topic model was used and how it was applied to the data set. | For example, multiple topic models were used and the report analyzes the differences between them. |\n",
    "> | **Task 1.1: Generated visualization** | The visualization is either absent or useless. | The visualization is present but does not help make clear what topics the people have talked about in the reviews. | The visualization clearly shows and distinguishes what topics people have talked about in the reviews. | For example, multiple visualizations were used and the report analyzes the comparative strengths of each.\n",
    "> | **Task 1.2: Generated sets of topics** | The two subsets are not comparable. | The two subsets are comparable. A topic model was used on the two subsets, but the report fails to mention what model was used and/or how it was applied to the data set. | The two subsets are comparable. The report clearly explains what topic model was used and how it was applied to the two subsets. | For example, multiple interesting subsets were identified and assessed for their usefulness, or multiple topic models were applied to the two subsets with differences between them analyzed.\n",
    "> | **Task 1.2: Visualization of comparison** | The two subsets are visualized in such a way that similarities and differences are not clear. | The two subsets are visualized in such a way to show the similarity of the two subsets, but no attempt was made to show the differences. | The two subsets are visualized in such a way that both similarities and differences are very apparent. | Extra transformation of the data was done to improve visualization, or multiple ways of visualizing the topics were used to provide a very comprehensive comparison.\n",
    "> | **Visualizations: Appropriateness of choice** | The visualization methods are not suitable for the type of data. | The visualization methods are suitable for the type of data, but another way to visualize the data is clearly better. | The visualization methods used are quite suitable for the type of data and made relationships clear. | Furthermore, extra effort was made to make the visualizations beautifully designed and/or usefully interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Set\n",
    "\n",
    "Note: I cleaned and saved a Pandas dataframe (as a GZIPped pickle) from the Yelp reviews dataset in a separate notebook \"task00-yelp-reviews-to-pandas-dataframe.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to data source, work in process (\"WIP\"), and output\n",
    "PATH_SOURCE = \"source/\"\n",
    "PATH_WIP = \"wip/\"\n",
    "PATH_OUTPUT = \"output/\"\n",
    "\n",
    "# Set review file path\n",
    "PATH_SOURCE_YELP_REVIEWS = PATH_SOURCE + \"yelp_academic_dataset_review.pkl.gzip\"\n",
    "PATH_WIP_TOKENIZER = PATH_WIP + \"task01_tokenizer.pkl\"\n",
    "PATH_WIP_TOKEN_MATRIX = PATH_WIP + \"task01_token_matrix.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pickled dataframe\n",
    "dfYelpReviews = pd.read_pickle(PATH_SOURCE_YELP_REVIEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1125458, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15SdjuK7DmYqUAj6rjGowg</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2007-05-17</td>\n",
       "      <td>5</td>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "      <td>review</td>\n",
       "      <td>Xqd0DzHaiyRqVH3WRG7hzg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF6UnRTtG7tWMcrO2GEoAg</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2010-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>review</td>\n",
       "      <td>H1kH6QZV7Le4zqTRNxoZow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-TsVN230RCkLYKBeLsuz7A</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-02-14</td>\n",
       "      <td>4</td>\n",
       "      <td>Dr. Goldberg has been my doctor for years and ...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dNocEAyUucjT371NNND41Q</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>4</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>KBLW4wJA_fwoWmMhiHRVOA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebcN2aqmNUuYNoyvQErgnA</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-05-15</td>\n",
       "      <td>4</td>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   business_id       date  stars  \\\n",
       "review_id                                                          \n",
       "15SdjuK7DmYqUAj6rjGowg  vcNAWiLM4dR7D2nwwJ7nCA 2007-05-17      5   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  vcNAWiLM4dR7D2nwwJ7nCA 2010-03-22      2   \n",
       "-TsVN230RCkLYKBeLsuz7A  vcNAWiLM4dR7D2nwwJ7nCA 2012-02-14      4   \n",
       "dNocEAyUucjT371NNND41Q  vcNAWiLM4dR7D2nwwJ7nCA 2012-03-02      4   \n",
       "ebcN2aqmNUuYNoyvQErgnA  vcNAWiLM4dR7D2nwwJ7nCA 2012-05-15      4   \n",
       "\n",
       "                                                                     text  \\\n",
       "review_id                                                                   \n",
       "15SdjuK7DmYqUAj6rjGowg  dr. goldberg offers everything i look for in a...   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  Unfortunately, the frustration of being Dr. Go...   \n",
       "-TsVN230RCkLYKBeLsuz7A  Dr. Goldberg has been my doctor for years and ...   \n",
       "dNocEAyUucjT371NNND41Q  Been going to Dr. Goldberg for over 10 years. ...   \n",
       "ebcN2aqmNUuYNoyvQErgnA  Got a letter in the mail last week that said D...   \n",
       "\n",
       "                          type                 user_id  votes_cool  \\\n",
       "review_id                                                            \n",
       "15SdjuK7DmYqUAj6rjGowg  review  Xqd0DzHaiyRqVH3WRG7hzg           1   \n",
       "RF6UnRTtG7tWMcrO2GEoAg  review  H1kH6QZV7Le4zqTRNxoZow           0   \n",
       "-TsVN230RCkLYKBeLsuz7A  review  zvJCcrpm2yOZrxKffwGQLA           1   \n",
       "dNocEAyUucjT371NNND41Q  review  KBLW4wJA_fwoWmMhiHRVOA           0   \n",
       "ebcN2aqmNUuYNoyvQErgnA  review  zvJCcrpm2yOZrxKffwGQLA           1   \n",
       "\n",
       "                        votes_funny  votes_useful  \n",
       "review_id                                          \n",
       "15SdjuK7DmYqUAj6rjGowg            0             2  \n",
       "RF6UnRTtG7tWMcrO2GEoAg            0             2  \n",
       "-TsVN230RCkLYKBeLsuz7A            0             1  \n",
       "dNocEAyUucjT371NNND41Q            0             0  \n",
       "ebcN2aqmNUuYNoyvQErgnA            0             2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataframe shape and head\n",
    "print(f\"Shape: {dfYelpReviews.shape}\")\n",
    "dfYelpReviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to load token matrix from file if found; set this to False when changing\n",
    "# other parameters\n",
    "load_token_matrix_from_file = True\n",
    "overwrite_saved_token_matrix = not load_token_matrix_from_file\n",
    "\n",
    "# Set token limit\n",
    "max_features = 10000\n",
    "\n",
    "# Set document frequency ceiling; topic analysis will ignore words found in more documents\n",
    "max_df = 0.5\n",
    "\n",
    "# Set document frequency floor; topic analysis will ignore words found in fewer document\n",
    "min_df = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tokenizer\n",
    "\n",
    "The `TfidVectorizer` class has a default pre-processor and tokenizer. While the pre-processing steps meet my needs (i.e., puncuation removal and setting lower-case) the tokenizer does not lemmatize nor stem words. Those two additional steps should produce more stable topics. I therefore create my own tokenizer.\n",
    "\n",
    "Note: I create `MyTokenizer` is a class to internalize instantiation of NLK's `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"String tokenizer utilizing lemmatizing and stemming.\"\"\"\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        \"\"\"Return tokens from a string.\"\"\"\n",
    "        return [self.wnl.lemmatize(token) for token in nltk.word_tokenize(document)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized TF-IDF\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer limiting \n",
    "vectorizer = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df, \\\n",
    "                            stop_words=\"english\", use_idf=True, tokenizer=MyTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working dataframe to a 30% sample of the full data set; too large otherwise\n",
    "df = dfYelpReviews.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing documents to build token matrix...\n",
      "CPU times: user 11min 25s, sys: 1.01 s, total: 11min 26s\n",
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load token matrix and vectorizer from file if found and flag set to permit;\n",
    "# otherwise vectorize documents\n",
    "tokenMatrix = None\n",
    "if(load_token_matrix_from_file and \\\n",
    "   os.path.isfile(PATH_WIP_TOKEN_MATRIX) and \n",
    "   os.path.isfile(PATH_WIP_TOKENIZER)):\n",
    "    print(f\"Loading token matrix from file \\\"{PATH_WIP_TOKEN_MATRIX}\\\"...\")\n",
    "    tokenMatrix = load_npz(PATH_WIP_TOKEN_MATRIX)\n",
    "    with open(PATH_WIP_TOKENIZER, \"r\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "        f.close()\n",
    "else:\n",
    "    print(\"Vectorizing documents to build token matrix...\")\n",
    "    tokenMatrix = vectorizer.fit_transform(df.text)\n",
    "    overwrite_saved_token_matrix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4,642 tokens in 337,637 documents\n"
     ]
    }
   ],
   "source": [
    "# Print token matrix shape\n",
    "print(\"Found {0[1]:,} tokens in {0[0]:,} documents\".format(tokenMatrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save token matrix and vectorizer to file if changed\n",
    "if(overwrite_saved_token_matrix):\n",
    "    save_npz(PATH_WIP_TOKEN_MATRIX, tokenMatrix)\n",
    "    with open(PATH_WIP_TOKENIZER, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Topics Using LDA\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics\n",
    "num_topics = 50\n",
    "\n",
    "# Set number of words to display for each topic\n",
    "num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GenSim corpus from token vectors\n",
    "corpus = matutils.Sparse2Corpus(tokenMatrix, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for token_id: token\n",
    "id2word = dict([(i, t) for i, t in enumerate(vectorizer.get_feature_names())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 14s, sys: 3.48 s, total: 3min 18s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find topics using LDA\n",
    "lda = models.ldamulticore.LdaMulticore(corpus, num_topics=num_topics, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"club\" + 0.008*\"drink\" + 0.007*\"n\\'t\" + 0.007*\"music\" + 0.007*\"night\" + 0.007*\"\\'s\" + 0.007*\"...\" + 0.007*\"dance\" + 0.007*\"place\" + 0.006*\"bar\"'),\n",
       " (1,\n",
       "  '0.013*\"dr.\" + 0.010*\"office\" + 0.009*\"doctor\" + 0.007*\"care\" + 0.007*\"staff\" + 0.006*\"n\\'t\" + 0.006*\"time\" + 0.006*\"dentist\" + 0.005*\"great\" + 0.005*\"patient\"'),\n",
       " (2,\n",
       "  '0.009*\"eyebrow\" + 0.007*\"wax\" + 0.007*\"n\\'t\" + 0.006*\"place\" + 0.006*\"\\'s\" + 0.006*\"waxing\" + 0.006*\"espresso\" + 0.006*\"food\" + 0.005*\"good\" + 0.005*\"great\"'),\n",
       " (3,\n",
       "  '0.018*\"breakfast\" + 0.013*\"egg\" + 0.007*\"good\" + 0.007*\"food\" + 0.006*\"hash\" + 0.006*\"toast\" + 0.006*\"place\" + 0.006*\"n\\'t\" + 0.006*\"pancake\" + 0.006*\"\\'s\"'),\n",
       " (4,\n",
       "  '0.041*\"pizza\" + 0.009*\"crust\" + 0.008*\"good\" + 0.007*\"\\'s\" + 0.007*\"place\" + 0.006*\"great\" + 0.006*\"n\\'t\" + 0.006*\"slice\" + 0.005*\"cheese\" + 0.005*\"...\"'),\n",
       " (5,\n",
       "  '0.017*\"food\" + 0.014*\"chinese\" + 0.012*\"good\" + 0.009*\"great\" + 0.009*\"place\" + 0.007*\"chicken\" + 0.007*\"noodle\" + 0.007*\"service\" + 0.006*\"\\'s\" + 0.006*\"restaurant\"'),\n",
       " (6,\n",
       "  '0.007*\"food\" + 0.006*\"n\\'t\" + 0.005*\"...\" + 0.005*\"good\" + 0.005*\"salad\" + 0.005*\"great\" + 0.005*\"place\" + 0.005*\"\\'s\" + 0.005*\"steak\" + 0.004*\")\"'),\n",
       " (7,\n",
       "  '0.009*\"breakfast\" + 0.008*\"brunch\" + 0.007*\"food\" + 0.007*\"\\'s\" + 0.007*\"pancake\" + 0.006*\"place\" + 0.006*\"good\" + 0.006*\"great\" + 0.005*\"n\\'t\" + 0.005*\")\"'),\n",
       " (8,\n",
       "  '0.007*\"place\" + 0.007*\"...\" + 0.007*\"\\'s\" + 0.006*\"n\\'t\" + 0.005*\"good\" + 0.005*\"food\" + 0.005*\")\" + 0.005*\"gelato\" + 0.005*\"just\" + 0.005*\"(\"'),\n",
       " (9,\n",
       "  '0.017*\"store\" + 0.008*\"\\'s\" + 0.007*\"n\\'t\" + 0.006*\"shop\" + 0.005*\"like\" + 0.005*\"item\" + 0.005*\"...\" + 0.005*\"place\" + 0.005*\")\" + 0.004*\"(\"'),\n",
       " (10,\n",
       "  '0.032*\"sushi\" + 0.017*\"roll\" + 0.009*\"place\" + 0.008*\"good\" + 0.007*\"great\" + 0.006*\"n\\'t\" + 0.006*\"ramen\" + 0.006*\"food\" + 0.006*\"fish\" + 0.006*\"tuna\"'),\n",
       " (11,\n",
       "  '0.014*\"buffet\" + 0.010*\"crab\" + 0.008*\"steak\" + 0.008*\"rib\" + 0.007*\"good\" + 0.007*\"food\" + 0.006*\"n\\'t\" + 0.006*\"lobster\" + 0.006*\"prime\" + 0.006*\"\\'s\"'),\n",
       " (12,\n",
       "  '0.037*\"burger\" + 0.016*\"fry\" + 0.008*\"good\" + 0.006*\"cheese\" + 0.006*\"n\\'t\" + 0.006*\"place\" + 0.006*\"great\" + 0.006*\"\\'s\" + 0.006*\"food\" + 0.005*\"onion\"'),\n",
       " (13,\n",
       "  '0.007*\"n\\'t\" + 0.006*\"car\" + 0.006*\"$\" + 0.005*\"did\" + 0.005*\"time\" + 0.005*\"...\" + 0.005*\"?\" + 0.005*\"called\" + 0.005*\"told\" + 0.005*\"\\'\\'\"'),\n",
       " (14,\n",
       "  '0.009*\"movie\" + 0.008*\"\\'s\" + 0.007*\"place\" + 0.006*\"pasty\" + 0.006*\"san\" + 0.006*\"food\" + 0.006*\"great\" + 0.006*\"n\\'t\" + 0.006*\"good\" + 0.005*\"theater\"'),\n",
       " (15,\n",
       "  '0.014*\"vegan\" + 0.011*\"love\" + 0.010*\"food\" + 0.009*\"great\" + 0.008*\"place\" + 0.008*\"gluten\" + 0.007*\"good\" + 0.006*\"\\'s\" + 0.006*\"...\" + 0.006*\"n\\'t\"'),\n",
       " (16,\n",
       "  '0.008*\"great\" + 0.008*\"food\" + 0.008*\"place\" + 0.007*\"fountain\" + 0.007*\"\\'s\" + 0.007*\"good\" + 0.006*\"bruschetta\" + 0.006*\"...\" + 0.005*\"yum\" + 0.005*\"bellagio\"'),\n",
       " (17,\n",
       "  '0.013*\"gym\" + 0.008*\"wedding\" + 0.006*\"\\'s\" + 0.006*\"n\\'t\" + 0.005*\"workout\" + 0.005*\"class\" + 0.005*\"equipment\" + 0.005*\"great\" + 0.004*\"place\" + 0.004*\"fitness\"'),\n",
       " (18,\n",
       "  '0.007*\"\\'s\" + 0.007*\"beer\" + 0.007*\"n\\'t\" + 0.006*\"great\" + 0.006*\"cirque\" + 0.005*\"stage\" + 0.005*\"seat\" + 0.005*\"ticket\" + 0.005*\"music\" + 0.005*\"place\"'),\n",
       " (19,\n",
       "  '0.013*\"food\" + 0.013*\"wing\" + 0.011*\"mexican\" + 0.009*\"good\" + 0.009*\"place\" + 0.008*\"great\" + 0.008*\"crepe\" + 0.007*\"boba\" + 0.006*\"\\'s\" + 0.006*\"salsa\"'),\n",
       " (20,\n",
       "  '0.019*\"sandwich\" + 0.009*\"bbq\" + 0.007*\"good\" + 0.007*\"\\'s\" + 0.006*\"place\" + 0.006*\"n\\'t\" + 0.005*\"chicken\" + 0.005*\")\" + 0.005*\"food\" + 0.005*\"sauce\"'),\n",
       " (21,\n",
       "  '0.024*\"donut\" + 0.017*\"class\" + 0.009*\"yoga\" + 0.008*\"studio\" + 0.007*\"instructor\" + 0.006*\"n\\'t\" + 0.006*\"\\'s\" + 0.006*\"great\" + 0.005*\"place\" + 0.005*\"...\"'),\n",
       " (22,\n",
       "  '0.009*\"mary\" + 0.009*\"bloody\" + 0.008*\"food\" + 0.007*\"tan\" + 0.007*\"falafel\" + 0.007*\"place\" + 0.007*\"great\" + 0.007*\"good\" + 0.006*\"...\" + 0.006*\"\\'s\"'),\n",
       " (23,\n",
       "  '0.010*\"game\" + 0.009*\"\\'s\" + 0.009*\"great\" + 0.009*\"place\" + 0.009*\"bar\" + 0.006*\"good\" + 0.006*\"n\\'t\" + 0.006*\"food\" + 0.006*\"sport\" + 0.005*\"beer\"'),\n",
       " (24,\n",
       "  '0.014*\"airport\" + 0.007*\"flight\" + 0.006*\"\\'s\" + 0.006*\"n\\'t\" + 0.005*\"terminal\" + 0.005*\"shuttle\" + 0.004*\"place\" + 0.004*\"time\" + 0.004*\"bus\" + 0.004*\"food\"'),\n",
       " (25,\n",
       "  '0.012*\"food\" + 0.010*\"great\" + 0.009*\"place\" + 0.009*\"good\" + 0.008*\"\\'s\" + 0.008*\"wine\" + 0.007*\"...\" + 0.007*\"service\" + 0.005*\"n\\'t\" + 0.005*\"oyster\"'),\n",
       " (26,\n",
       "  '0.007*\"m\" + 0.007*\"\\'s\" + 0.006*\"place\" + 0.006*\"n\\'t\" + 0.005*\"brow\" + 0.005*\"great\" + 0.005*\"...\" + 0.005*\"food\" + 0.005*\"ipa\" + 0.004*\"good\"'),\n",
       " (27,\n",
       "  '0.006*\")\" + 0.006*\"(\" + 0.005*\"n\\'t\" + 0.005*\"good\" + 0.005*\"food\" + 0.005*\"\\'s\" + 0.005*\"wine\" + 0.005*\"-\" + 0.005*\"restaurant\" + 0.005*\"...\"'),\n",
       " (28,\n",
       "  '0.013*\"sub\" + 0.007*\"\\'s\" + 0.006*\"great\" + 0.006*\"n\\'t\" + 0.006*\"good\" + 0.005*\"food\" + 0.005*\"target\" + 0.005*\"place\" + 0.005*\"...\" + 0.005*\"time\"'),\n",
       " (29,\n",
       "  '0.015*\"bagel\" + 0.008*\"cream\" + 0.007*\"n\\'t\" + 0.007*\"ice\" + 0.006*\"\\'s\" + 0.006*\"place\" + 0.006*\"food\" + 0.006*\"good\" + 0.005*\"...\" + 0.005*\"great\"'),\n",
       " (30,\n",
       "  '0.023*\"dog\" + 0.009*\"nail\" + 0.007*\"great\" + 0.007*\"gel\" + 0.007*\"n\\'t\" + 0.006*\"place\" + 0.006*\"time\" + 0.006*\"manicure\" + 0.006*\"\\'s\" + 0.005*\"store\"'),\n",
       " (31,\n",
       "  '0.007*\"\\'s\" + 0.006*\"park\" + 0.006*\"place\" + 0.006*\"great\" + 0.005*\"n\\'t\" + 0.005*\"trail\" + 0.004*\"tour\" + 0.004*\")\" + 0.004*\"time\" + 0.004*\"(\"'),\n",
       " (32,\n",
       "  '0.021*\"taco\" + 0.014*\"burrito\" + 0.010*\"salsa\" + 0.010*\"mexican\" + 0.010*\"food\" + 0.008*\"good\" + 0.008*\"place\" + 0.008*\"carne\" + 0.008*\"asada\" + 0.007*\"\\'s\"'),\n",
       " (33,\n",
       "  '0.009*\"yogurt\" + 0.009*\"gyro\" + 0.008*\"place\" + 0.007*\"good\" + 0.007*\"food\" + 0.007*\"\\'s\" + 0.006*\"chicken\" + 0.006*\"salad\" + 0.006*\"n\\'t\" + 0.005*\"flavor\"'),\n",
       " (34,\n",
       "  '0.007*\"\\'s\" + 0.005*\"good\" + 0.005*\"place\" + 0.005*\"food\" + 0.005*\"n\\'t\" + 0.005*\")\" + 0.005*\"great\" + 0.004*\"...\" + 0.004*\"(\" + 0.004*\"like\"'),\n",
       " (35,\n",
       "  '0.008*\"\\'s\" + 0.008*\"place\" + 0.007*\"food\" + 0.007*\"good\" + 0.006*\"latte\" + 0.006*\"n\\'t\" + 0.006*\"great\" + 0.006*\"waffle\" + 0.005*\"love\" + 0.005*\"coffee\"'),\n",
       " (36,\n",
       "  '0.016*\"hair\" + 0.012*\"car\" + 0.008*\"great\" + 0.008*\"job\" + 0.007*\"cut\" + 0.007*\"massage\" + 0.006*\"work\" + 0.006*\"n\\'t\" + 0.006*\"salon\" + 0.006*\"haircut\"'),\n",
       " (37,\n",
       "  '0.015*\"food\" + 0.012*\"service\" + 0.010*\"u\" + 0.009*\"minute\" + 0.008*\"n\\'t\" + 0.008*\"order\" + 0.007*\"time\" + 0.007*\"table\" + 0.007*\"server\" + 0.006*\"good\"'),\n",
       " (38,\n",
       "  '0.030*\"thai\" + 0.012*\"curry\" + 0.012*\"pad\" + 0.011*\"food\" + 0.008*\"place\" + 0.007*\"good\" + 0.006*\"great\" + 0.006*\"n\\'t\" + 0.006*\"\\'s\" + 0.005*\"spicy\"'),\n",
       " (39,\n",
       "  '0.022*\"pho\" + 0.008*\"good\" + 0.007*\"dim\" + 0.007*\"place\" + 0.007*\"sum\" + 0.007*\"food\" + 0.006*\"\\'s\" + 0.006*\"n\\'t\" + 0.006*\")\" + 0.005*\"roll\"'),\n",
       " (40,\n",
       "  '0.012*\"indian\" + 0.012*\"food\" + 0.008*\"good\" + 0.007*\"great\" + 0.007*\"place\" + 0.006*\"chicken\" + 0.006*\"restaurant\" + 0.006*\"service\" + 0.006*\"naan\" + 0.006*\"greek\"'),\n",
       " (41,\n",
       "  '0.007*\"\\'s\" + 0.007*\"place\" + 0.006*\"food\" + 0.006*\"great\" + 0.006*\"n\\'t\" + 0.006*\"good\" + 0.005*\"cooky\" + 0.005*\"smoothy\" + 0.004*\"stadium\" + 0.004*\")\"'),\n",
       " (42,\n",
       "  '0.010*\"irish\" + 0.007*\"food\" + 0.006*\"doughnut\" + 0.006*\"n\\'t\" + 0.006*\"place\" + 0.006*\"great\" + 0.006*\"...\" + 0.006*\"\\'s\" + 0.006*\"pub\" + 0.006*\"service\"'),\n",
       " (43,\n",
       "  '0.008*\"\\'s\" + 0.008*\"karaoke\" + 0.007*\"place\" + 0.007*\"n\\'t\" + 0.007*\"food\" + 0.006*\"...\" + 0.006*\"good\" + 0.005*\"great\" + 0.005*\")\" + 0.005*\"bar\"'),\n",
       " (44,\n",
       "  '0.012*\"nail\" + 0.008*\"massage\" + 0.007*\"spa\" + 0.007*\"pedicure\" + 0.006*\"n\\'t\" + 0.006*\"bike\" + 0.006*\"time\" + 0.005*\"did\" + 0.005*\"place\" + 0.005*\"service\"'),\n",
       " (45,\n",
       "  '0.018*\"cupcake\" + 0.012*\"cake\" + 0.007*\"...\" + 0.007*\"\\'s\" + 0.006*\"n\\'t\" + 0.006*\"frosting\" + 0.006*\"place\" + 0.005*\"good\" + 0.005*\")\" + 0.005*\"like\"'),\n",
       " (46,\n",
       "  '0.020*\"great\" + 0.020*\"food\" + 0.016*\"service\" + 0.011*\"excellent\" + 0.011*\"place\" + 0.009*\"good\" + 0.009*\"staff\" + 0.009*\"friendly\" + 0.008*\"awesome\" + 0.007*\"time\"'),\n",
       " (47,\n",
       "  '0.023*\"room\" + 0.017*\"hotel\" + 0.009*\"stay\" + 0.009*\"pool\" + 0.008*\"casino\" + 0.007*\"strip\" + 0.007*\"\\'s\" + 0.006*\"n\\'t\" + 0.006*\"stayed\" + 0.006*\"nice\"'),\n",
       " (48,\n",
       "  '0.018*\"coffee\" + 0.011*\"starbucks\" + 0.009*\"place\" + 0.008*\"great\" + 0.007*\"food\" + 0.007*\"=\" + 0.007*\"\\'s\" + 0.007*\"good\" + 0.006*\"n\\'t\" + 0.006*\"...\"'),\n",
       " (49,\n",
       "  '0.008*\"chicken\" + 0.007*\"rice\" + 0.007*\"good\" + 0.007*\"food\" + 0.006*\"n\\'t\" + 0.006*\"fried\" + 0.005*\")\" + 0.005*\"shrimp\" + 0.005*\"sauce\" + 0.005*\"(\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print topics\n",
    "lda.show_topics(num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
