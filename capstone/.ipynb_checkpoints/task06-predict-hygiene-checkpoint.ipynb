{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loganjtravis@gmail.com (Logan Travis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "# Imports; captures errors to supress warnings about changing\n",
    "# import syntax\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Set random seed for repeatability\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "From course page [Week 5 > Task 6 Information > Task 6 Overview](https://www.coursera.org/learn/data-mining-project/supplement/gvCsC/task-4-and-5-overview):\n",
    "\n",
    "> In this task, you are going to predict whether a set of restaurants will pass the public health inspection tests given the corresponding Yelp text reviews along with some additional information such as the locations and cuisines offered in these restaurants. Making a prediction about an unobserved attribute using data mining techniques represents a wide range of important applications of data mining. Through working on this task, you will gain direct experience with such an application. Due to the flexibility of using as many indicators for prediction as possible, this would also give you an opportunity to potentially combine many different algorithms you have learned from the courses in the Data Mining Specialization to solve a real world problem and experiment with different methods to understand what’s the most effective way of solving the problem.\n",
    "> \n",
    "> **About the Dataset**\n",
    "You should first [download the dataset](https://d396qusza40orc.cloudfront.net/dataminingcapstone/Task6/Hygiene.tar.gz). The dataset is composed of a training subset containing 546 restaurants used for training your classifier, in addition to a testing subset of 12753 restaurants used for evaluating the performance of the classifier. In the training subset, you will be provided with a binary label for each restaurant, which indicates whether the restaurant has passed the latest public health inspection test or not, whereas for the testing subset, you will not have access to any labels. The dataset is spread across three files such that the first 546 lines in each file correspond to the training subset, and the rest are part of the testing subset. Below is a description of each file:\n",
    ">\n",
    "> * hygiene.dat: Each line contains the concatenated text reviews of one restaurant.\n",
    "> * hygiene.dat.labels: For the first 546 lines, a binary label (0 or 1) is used where a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test. The rest of the lines have \"[None]\" in their label field implying that they are part of the testing subset.\n",
    "> * hygiene.dat.additional: It is a CSV (Comma-Separated Values) file where the first value is a list containing the cuisines offered, the second value is the zip code, which gives an idea about the location, the third is the number of reviews, and the fourth is the average rating, which can vary between 0 and 5 (5 being the best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "source": [
    "# A Note on The Training Data\n",
    "\n",
    "I realized after building my second model that I incorrectly interpreted the meaning of `hygiene.data.labels`. The assignment states, \"...a 0 indicates that the restaurant has passed the latest public health inspection test, while a 1 means that the restaurant has failed the test.\" It does not indicate the immediacy (in time) of those last inspections. A restaurant might have failed its hygiene inspection only days before compiling the data set as easily as another restaurant passed an inspection from years ago. Also, the reviews lack time indicators so a restaurant's reviews might include reviews from a decade ago when it failed a hygiene inspection. How those reviews affect the prediction of future failure would depend on many factors.\n",
    "\n",
    "**In short:** I recommend tempering expectations for accurate prediction. Even if a model works, it will necessarily overfit to the nuances of this training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 01: Logistic Regression of Unigram Probability\n",
    "\n",
    "I start by representing text by creating a unigram form each restaurant's reviews then apply logistic regression. This simple model gives a useful baseline for future methods. It also highlights the difficulty of the prediction: Logistic regression alone proves an *incredibly* poor predictor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Set paths to data source, work in process (\"WIP\"), and output\n",
    "PATH_SOURCE = \"source\"\n",
    "PATH_WIP = \"wip\"\n",
    "PATH_OUTPUT = \"output\"\n",
    "\n",
    "# Set file paths\n",
    "PATH_SOURCE_TRAIN_TEXT = f\"{PATH_SOURCE}/Hygiene/train_hygiene.dat\"\n",
    "PATH_SOURCE_TRAIN_LABELS = f\"{PATH_SOURCE}/Hygiene/train_hygiene.dat.labels\"\n",
    "PATH_SOURCE_TRAIN_REST = f\"{PATH_SOURCE}/Hygiene/train_hygiene.dat.additional\"\n",
    "PATH_SOURCE_TARGET_TEXT = f\"{PATH_SOURCE}/Hygiene/target_hygiene.dat\"\n",
    "PATH_SOURCE_TARGET_REST = f\"{PATH_SOURCE}/Hygiene/target_hygiene.dat.additional\"\n",
    "\n",
    "# Set paths to AutoPhrase output\n",
    "AUTOPHRASE_LOG = \"AutoPhrase/models/hygiene/log.txt\"\n",
    "AUTOPHRASE_RESULTS = \"AutoPhrase/models/hygiene/AutoPhrase.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get training text and labels\n",
    "with open(PATH_SOURCE_TRAIN_TEXT) as f:\n",
    "    arrTrainText = [l.rstrip() for l in f]\n",
    "with open(PATH_SOURCE_TRAIN_LABELS) as f:\n",
    "    arrTrainLabels = [l.rstrip() == \"1\" for l in f]\n",
    "dfTrain = pd.DataFrame(data={\"failed_hygiene\": arrTrainLabels, \"review_text\": arrTrainText})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "dfTrain[\"review_text_len\"] = dfTrain.review_text.str.len()\n",
    "dfTrain, dfTest = train_test_split(dfTrain, test_size=0.3, random_state=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failed_hygiene</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>True</td>\n",
       "      <td>Lovely place! Great neighborhood feel, excelle...</td>\n",
       "      <td>17352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>False</td>\n",
       "      <td>The Crab Spring rolls were absolutely amazing!...</td>\n",
       "      <td>12390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>False</td>\n",
       "      <td>We went about a year ago... the experience was...</td>\n",
       "      <td>3107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>False</td>\n",
       "      <td>I was expecting a lot more given all the great...</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>False</td>\n",
       "      <td>This joint became a regular stop for us when w...</td>\n",
       "      <td>4765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     failed_hygiene                                        review_text  \\\n",
       "463            True  Lovely place! Great neighborhood feel, excelle...   \n",
       "240           False  The Crab Spring rolls were absolutely amazing!...   \n",
       "461           False  We went about a year ago... the experience was...   \n",
       "257           False  I was expecting a lot more given all the great...   \n",
       "407           False  This joint became a regular stop for us when w...   \n",
       "\n",
       "     review_text_len  \n",
       "463            17352  \n",
       "240            12390  \n",
       "461             3107  \n",
       "257             2566  \n",
       "407             4765  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect first 5 rows\n",
    "dfTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Statistics\n",
      "-----\n",
      "               review_text review_text_len              \n",
      "                     count            mean           std\n",
      "failed_hygiene                                          \n",
      "False                  195     7276.015385  10327.798184\n",
      "True                   187     9967.219251  12589.871449\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on training versus testing split\n",
    "print(\"Training Data Statistics\\n-----\")\n",
    "print(dfTrain.groupby([\"failed_hygiene\"]).agg({\n",
    "    \"review_text\": [\"count\"],\n",
    "    \"review_text_len\": [\"mean\", \"std\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Statistics\n",
      "-----\n",
      "               review_text review_text_len              \n",
      "                     count            mean           std\n",
      "failed_hygiene                                          \n",
      "False                   78     6230.256410   8406.959927\n",
      "True                    86     9252.313953  10821.455737\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on training versus testing split\n",
    "print(\"Testing Data Statistics\\n-----\")\n",
    "print(dfTest.groupby([\"failed_hygiene\"]).agg({\n",
    "    \"review_text\": [\"count\"],\n",
    "    \"review_text_len\": [\"mean\", \"std\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unigram Probability Matrix\n",
    "\n",
    "I chose not to use IDF weighting. The training data concatenates all reviews for a restaurant with no delimiter to split them. Applying IDF instead of penalizing frequent terms across *documents* would penalize them across *restaurants*. This does not properly scale frequent terms.\n",
    "\n",
    "Consider a term that appears once in every review versus a term that appears multiple times in one review for each restaurant. The term that appears in one review multiple times likely has greater predictive value than the term that appears in every review once. Unfortunately, penalizing by appearance in concatenated restaurant reviews would scale the two terms equally.\n",
    "\n",
    "I instead count terms then normalize appearances within each restaurant creating a term probability matrix. Several additional comments:\n",
    "\n",
    "* The [SciKit Learn `CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) does not lemmatize nor stem terms by default. I create my own tokenizer class to add those pre-processing steps.\n",
    "* Logistic regression works best when the number of samples far exceeds the number of features. That is not the case for this data set. Expect poor performance with high sensitivity to model parameters.\n",
    "* I did not tune the term vectorizer; it includes all terms found in the training data.\n",
    "* I will tune the parameters in the next model. I intend this model as a *naïve* baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"String tokenizer utilizing lemmatizing and stemming.\"\"\"\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, document):\n",
    "        \"\"\"Return tokens from a string.\"\"\"\n",
    "        return [self.wnl.lemmatize(token) for \\\n",
    "                        token in nltk.word_tokenize(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF vectorizer \n",
    "tf = CountVectorizer(max_df=1.0, min_df=1, \\\n",
    "                     stop_words=\"english\", \\\n",
    "                     tokenizer=MyTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.78 s, sys: 266 ms, total: 8.05 s\n",
      "Wall time: 8.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate training term frequencies\n",
    "trainTerms = tf.fit_transform(dfTrain.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 restaurant reviews extracted into 23,107 unigram terms.\n"
     ]
    }
   ],
   "source": [
    "# Normalize for each restaurant\n",
    "trainP = trainTerms / trainTerms.sum(axis=1)\n",
    "print(\"{:,} restaurant reviews extracted into {:,} unigram terms.\".format(*trainP.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.45 s, sys: 0 ns, total: 2.45 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate testing term frequencies; Note: Transform ONLY,\n",
    "# no additional fitting\n",
    "testTerms = tf.transform(dfTest.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Normalize for each restaurant\n",
    "testP = testTerms / testTerms.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model\n",
    "model_TF_LR = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 15.6 ms, total: 62.5 ms\n",
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train logistic regression model\n",
    "model_TF_LR = model_TF_LR.fit(trainP, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def printModelF1(truth, prediction, modelName, includeConfusionMatrix=True):\n",
    "    \"\"\"Print model quality using specified measure.\"\"\"\n",
    "    f1 = f1_score(truth, prediction)\n",
    "    print(\"{}\\n-----\\nF-1 Score: {:.6f}\".format(modelName, f1))\n",
    "    if(includeConfusionMatrix):\n",
    "        cm = confusion_matrix(truth, prediction)\n",
    "        print(\"True Negatives: {0:,}\\nTrue Positives: {3:,}\\nFalse Negatives: {2:,}\\nFalse Positives: {1:,}\".format(*cm.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 01: Logistic Regression of Unigram Probability\n",
      "-----\n",
      "F-1 Score: 0.000000\n",
      "True Negatives: 77\n",
      "True Positives: 0\n",
      "False Negatives: 86\n",
      "False Positives: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_TF_LR_Pred = model_TF_LR.predict(testP)\n",
    "printModelF1(dfTest.failed_hygiene, model_TF_LR_Pred, \\\n",
    "             \"Model 01: Logistic Regression of Unigram Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple logistic regression performed *horribly*. It only predict **one** failed hygiene inspection in the test data and that proved a false positive. I also tried other random seeds for the training/testing split. Some testing data sets yielded no predicted failed hygiene inspections.\n",
    "\n",
    "Review text simply includes too much noise. While we anticipate hygiene issues to appear in reviews, we should also expect them to drown in a sea of non-hygiene related reviews about the food, service, location, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model 02: Recursive Feature Elimination Before Logistic Regression\n",
    "\n",
    "I next try a feature selection method called Recursive Feature Elimination ([`RFE` on SciKit Learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.get_support)). The 22,000+ terms found in the training data far exceed the training sample (382). If a simple logistic regression has predictive value, it first needs to train on only the most useful features. Those features should also have strict independence.\n",
    "\n",
    "RFE will not directly consider independence but it can *quickly* reduce the number of features to the most useful. I find the 30 top ranked features as a starting point to see what improvements logistic regression has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Most Predictive Terms using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model for RFE\n",
    "model_TF_LR_RFE = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Recursive Feature Elimination instance\n",
    "rfe_TF_LR_RFE = RFE(model_TF_LR_RFE, n_features_to_select=30, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 16.4 s, total: 1min 28s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reduce features using Recursive Feature Elimination\n",
    "rfe_TF_LR_RFE = rfe_TF_LR_RFE.fit(trainP, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Restrict training terms to best from RFE and calculate new\n",
    "# relative probabilities\n",
    "trainTerms_RFE = trainTerms[:, rfe_TF_LR_RFE.get_support(indices=True)]\n",
    "trainP_RFE = trainTerms_RFE / trainTerms_RFE.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Restrict testing terms to best from RFE and calculate new\n",
    "# relative probabilities\n",
    "testTerms_RFE = testTerms[:, rfe_TF_LR_RFE.get_support(indices=True)]\n",
    "testP_RFE = testTerms_RFE / testTerms_RFE.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression after RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\n",
      "Wall time: 4.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train logistic regression model\n",
    "model_TF_LR_RFE = model_TF_LR_RFE.fit(trainP_RFE, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 02: Recursive Feature Elimination Before Logistic Regression\n",
      "-----\n",
      "F-1 Score: 0.442857\n",
      "True Negatives: 55\n",
      "True Positives: 31\n",
      "False Negatives: 55\n",
      "False Positives: 23\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_TF_LR_RFE_Pred = model_TF_LR_RFE.predict(testP_RFE)\n",
    "printModelF1(dfTest.failed_hygiene, model_TF_LR_RFE_Pred, \\\n",
    "             \"Model 02: Recursive Feature Elimination Before Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricting the logistic model to the 30 best terms improves its performance significantly. The resulting logistic model still performs about as well as a fair coin flip. A better technique to generate the most predictive features should improve model quality further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 03: Latent Symantic Analysis of Unigram Frequency Before Logistic Regression\n",
    "\n",
    "RFE selects the best features from an existing data set. The features it removes do not predict *as well* as the features it keeps but they can still have predictive value. Additionally, the remaining features may have significant covariance that would reduce the quality of logistic regression.\n",
    "\n",
    "I therefore try a feature decomposition method called Latent Symantic Analysis ([`TruncatedSVD` in SciKit Learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD)). The decomposition process still reduces the number of features but does so by linear combination of those features. Some ability to explain variation still gets loss. Usually much less than feature selection. The resulting decomposed features also have significantly less covariance.\n",
    "\n",
    "I tuned the number of decomposed features to 180. LSA frequently starts with 100 but I found - through trial and error - 180 produced the best results.\n",
    "\n",
    "**Note:** I perform LSA on the unigram frequency not probability within each restaurant's reviews. LSA on the unigram probabilities yielded worse results. Using frequencies does present a problem that I discuss in summarizing this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform LSA of Unigram Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Latent Semantic Analysis instance\n",
    "decomp_LSA = TruncatedSVD(n_components=180, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.08 s, sys: 1.06 s, total: 6.14 s\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Perform Latent Semantic Analysis on training terms\n",
    "decomp_LSA = decomp_LSA.fit(trainTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw training term counts into the\n",
    "# LSA decomposed features\n",
    "trainTermsLSA = decomp_LSA.transform(trainTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw testing term counts into the\n",
    "# LSA decomposed features\n",
    "testTermsLSA = decomp_LSA.transform(testTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model from LSA\n",
    "model_LSA_LR = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression on LSA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 0 ns, total: 46.9 ms\n",
      "Wall time: 47.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train logistic regression model on LSA features\n",
    "model_LSA_LR = model_LSA_LR.fit(trainTermsLSA, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 03: Latent Symantic Analysis of Unigram Frequency Before Logistic Regression\n",
      "-----\n",
      "F-1 Score: 0.674419\n",
      "True Negatives: 50\n",
      "True Positives: 58\n",
      "False Negatives: 28\n",
      "False Positives: 28\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_LSA_LR_Pred = model_LSA_LR.predict(testTermsLSA)\n",
    "printModelF1(dfTest.failed_hygiene, model_LSA_LR_Pred, \\\n",
    "             \"Model 03: Latent Symantic Analysis of Unigram Frequency Before Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression on the LSA features yields reasonable predictive value. I would recommend this model to a county hygiene inspector especially one with more restaurants to inspect than time. It has a higher than ideal false negative rate (predicting no hygiene issues when restaurant would fail its next inspection) **but** the model scales well. All the calculations can run in parallel after initial training even updating the unigram frequencies for a restaurant.\n",
    "\n",
    "The model *may* require regular re-training. Term frequencies should increase over time as a restaurant receives more reviews. The logistic regression coefficients *may* therefore need adjustment to the higher term frequencies.\n",
    "\n",
    "I emphasize \"may\" because the success of this model raised a question: Do most restaurants fail hygiene inspections earlier in their history? The training data does not include the date a restaurant opened so I cannot answer that question. However, we can hypothesize that the term frequency LSA into logistic regression model tips toward failed hygiene inspection on overall term frequencies as opposed to the frequencies of specific, more predictive terms. Diving into the LSA components and related terms exceeds the scope of this assignment (to build predictive models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 04: Latent Symantic Analysis of Phrase Frequency Before Logistic Regression\n",
    "\n",
    "Representing text as unigrams or even LSA features from those unigram vectors may not capture the best indicators. A phrase like \"greasy glass\" suggests bad hygiene more than \"greasy\" and \"glass\" do separately. I therefore generate a list of frequent phrases (up to trigrams) using [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase)\\[1\\]\\[2\\], an improved version of [SegPhrase](https://github.com/shangjingbo1226/SegPhrase). I then feed the frequency of those phrases through a similar LSA and logistic regression to compare with the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) uses Java which I cannot easily run inside this notebook. Please see below for my command line parameters and execution log.\n",
    "\n",
    "```bash\n",
    "MODEL='./models/hygiene' RAW_TRAIN='./wip/train_hygiene.dat' RAW_LABEL_FILE='./wip/train_hygiene.dat.labels' ./auto_phrase.sh 2>&1 | tee ./models/hygiene/log.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b(B\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b(B\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\n",
      "real\t0m1.996s\n",
      "user\t0m5.688s\n",
      "sys\t0m0.797s\n",
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing stopword file...\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "Current step: Tokenizing expert labels...\u001b[0K\n",
      "com.cybozu.labs.langdetect.LangDetectException: no features in text\n",
      "\tat com.cybozu.labs.langdetect.Detector.detectBlock(Detector.java:235)\n",
      "\tat com.cybozu.labs.langdetect.Detector.getProbabilities(Detector.java:221)\n",
      "\tat com.cybozu.labs.langdetect.Detector.detect(Detector.java:209)\n",
      "\tat Tokenizer.detectLanguage(Tokenizer.java:151)\n",
      "\tat Tokenizer.main(Tokenizer.java:824)\n",
      "Using default setting for unknown languages...\n",
      "Using default setting for unknown languages...\n",
      "Using default setting for unknown languages...\n",
      "Using default setting for unknown languages...\n",
      "Using default setting for unknown languages...\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b(B\u001b[m\n",
      "Current step: Splitting files...\u001b[0K\n",
      "Current step: Tagging...\u001b[0K\n",
      "Current step: Merging...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b(B\u001b[m\n",
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 1024854\n",
      "max word token id = 27087\n",
      "# of documents = 546\n",
      "# of distinct POS tags = 57\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 27091\n",
      "# of frequent phrases = 34509\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 3409\n",
      "\tThe size of the negative pool = 30845\n",
      "# truth patterns = 49379\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m8.071s\n",
      "user\t0m18.734s\n",
      "sys\t0m3.109s\n",
      "\u001b[32m===Saving Model and Results===\u001b(B\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b(B\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print AutoPhrase log file\n",
    "with open(AUTOPHRASE_LOG, \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Read AutoPhrase frequent phrases into dataframe\n",
    "dfPhrases = pd.read_csv(AUTOPHRASE_RESULTS, sep=\"\\t\", \\\n",
    "                        names=[\"score\", \"phrase\"], index_col=\"phrase\")\n",
    "dfPhrases.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert phrase dataframe to vocabulary dictionary for\n",
    "# use in `CountVectorizer`\n",
    "phrases = dfPhrases.phrase.to_dict() # {index:dish}\n",
    "phrases = {v: k for k, v in phrases.items()} # {dish:index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create phrase frequency vectorizer \n",
    "pf = CountVectorizer(max_df=1.0, min_df=1, \\\n",
    "                     stop_words=\"english\", \\\n",
    "                     tokenizer=MyTokenizer(), \\\n",
    "                     vocabulary=phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.91 s, sys: 31.2 ms, total: 6.94 s\n",
      "Wall time: 6.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate training phrase frequencies\n",
    "trainPhrases = pf.fit_transform(dfTrain.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 0 ns, total: 2.66 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate testing phrase frequencies; Note: Transform ONLY,\n",
    "# no additional fitting\n",
    "testPhrases = pf.transform(dfTest.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Latent Semantic Analysis instance\n",
    "decomp_LSAPhrases = TruncatedSVD(n_components=180, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 891 ms, sys: 62.5 ms, total: 953 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Perform Latent Semantic Analysis on training phrases\n",
    "decomp_LSAPhrases = decomp_LSAPhrases.fit(trainPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw training phrase counts into the\n",
    "# LSA decomposed features\n",
    "trainPhrasesLSA = decomp_LSAPhrases.transform(trainPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw testing phrase counts into the\n",
    "# LSA decomposed features\n",
    "testPhrasesLSA = decomp_LSAPhrases.transform(testPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model from phrase LSA\n",
    "model_Phrase_LSA_LR = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 0 ns, total: 125 ms\n",
      "Wall time: 39.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train logistic regression model on phrase LSA features\n",
    "model_Phrase_LSA_LR = model_LSA_LR.fit(trainPhrasesLSA, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 04: Latent Symantic Analysis of Phrase Frequency Before Logistic Regression\n",
      "-----\n",
      "F-1 Score: 0.647059\n",
      "True Negatives: 49\n",
      "True Positives: 55\n",
      "False Negatives: 31\n",
      "False Positives: 29\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_Phrase_LSA_LR_Pred = model_Phrase_LSA_LR.predict(testPhrasesLSA)\n",
    "printModelF1(dfTest.failed_hygiene, model_Phrase_LSA_LR_Pred, \\\n",
    "             \"Model 04: Latent Symantic Analysis of Phrase Frequency Before Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using frequent phrases (up to trigrams) instead of unigrams does not significantly alter the effectiveness of the model. Logistic regression on review text *alone* likely cannot improve further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 05: Append Restaurant Features to LSA of Unigram Frequency\n",
    "\n",
    "The training data includes additional features for each restaurant: categories, zip code, review count, and mean review score. I append the review count and mean review score to the previously generated LSA components from unigram frequency. Including those non-review features may improve logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Additional Features for Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get additional features for restaurants\n",
    "dfRestAdd = pd.read_csv(PATH_SOURCE_TRAIN_REST, sep=\",\", names=[\n",
    "    \"categories\",\n",
    "    \"zip_code\",\n",
    "    \"review_count\",\n",
    "    \"review_score_mean\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge additional columns into training and testing\n",
    "# data sets\n",
    "dfTrain = dfTrain.merge(dfRestAdd, how=\"left\", left_index=True, right_index=True)\n",
    "dfTest = dfTest.merge(dfRestAdd, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>failed_hygiene</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_len</th>\n",
       "      <th>categories</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_score_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>True</td>\n",
       "      <td>Lovely place! Great neighborhood feel, excelle...</td>\n",
       "      <td>17352</td>\n",
       "      <td>['French', 'Restaurants']</td>\n",
       "      <td>98112</td>\n",
       "      <td>23</td>\n",
       "      <td>3.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>False</td>\n",
       "      <td>The Crab Spring rolls were absolutely amazing!...</td>\n",
       "      <td>12390</td>\n",
       "      <td>['Seafood', 'Restaurants']</td>\n",
       "      <td>98109</td>\n",
       "      <td>14</td>\n",
       "      <td>3.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>False</td>\n",
       "      <td>We went about a year ago... the experience was...</td>\n",
       "      <td>3107</td>\n",
       "      <td>['Italian', 'Basque', 'Spanish', 'Restaurants']</td>\n",
       "      <td>98102</td>\n",
       "      <td>4</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>False</td>\n",
       "      <td>I was expecting a lot more given all the great...</td>\n",
       "      <td>2566</td>\n",
       "      <td>['Chinese', 'Restaurants']</td>\n",
       "      <td>98105</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>False</td>\n",
       "      <td>This joint became a regular stop for us when w...</td>\n",
       "      <td>4765</td>\n",
       "      <td>['Creperies', 'Food Stands', 'Restaurants']</td>\n",
       "      <td>98101</td>\n",
       "      <td>8</td>\n",
       "      <td>4.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     failed_hygiene                                        review_text  \\\n",
       "463            True  Lovely place! Great neighborhood feel, excelle...   \n",
       "240           False  The Crab Spring rolls were absolutely amazing!...   \n",
       "461           False  We went about a year ago... the experience was...   \n",
       "257           False  I was expecting a lot more given all the great...   \n",
       "407           False  This joint became a regular stop for us when w...   \n",
       "\n",
       "     review_text_len                                       categories  \\\n",
       "463            17352                        ['French', 'Restaurants']   \n",
       "240            12390                       ['Seafood', 'Restaurants']   \n",
       "461             3107  ['Italian', 'Basque', 'Spanish', 'Restaurants']   \n",
       "257             2566                       ['Chinese', 'Restaurants']   \n",
       "407             4765      ['Creperies', 'Food Stands', 'Restaurants']   \n",
       "\n",
       "     zip_code  review_count  review_score_mean  \n",
       "463     98112            23           3.782609  \n",
       "240     98109            14           3.533333  \n",
       "461     98102             4           3.250000  \n",
       "257     98105             3           3.000000  \n",
       "407     98101             8           4.625000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect first 5 rows\n",
    "dfTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Statistics\n",
      "-----\n",
      "               review_text review_text_len               review_count  \\\n",
      "                     count            mean           std         mean   \n",
      "failed_hygiene                                                          \n",
      "False                  195     7276.015385  10327.798184    10.651282   \n",
      "True                   187     9967.219251  12589.871449    14.620321   \n",
      "\n",
      "                          review_score_mean            \n",
      "                      std              mean       std  \n",
      "failed_hygiene                                         \n",
      "False           15.645935          3.598666  0.776270  \n",
      "True            17.182456          3.609714  0.751327  \n"
     ]
    }
   ],
   "source": [
    "# Sanity check on training versus testing split\n",
    "print(\"Training Data Statistics\\n-----\")\n",
    "print(dfTrain.groupby([\"failed_hygiene\"]).agg({\n",
    "    \"review_text\": [\"count\"],\n",
    "    \"review_text_len\": [\"mean\", \"std\"],\n",
    "    \"review_count\": [\"mean\", \"std\"],\n",
    "    \"review_score_mean\": [\"mean\", \"std\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Statistics\n",
      "-----\n",
      "               review_text review_text_len               review_count  \\\n",
      "                     count            mean           std         mean   \n",
      "failed_hygiene                                                          \n",
      "False                   78     6230.256410   8406.959927     9.115385   \n",
      "True                    86     9252.313953  10821.455737    13.337209   \n",
      "\n",
      "                          review_score_mean            \n",
      "                      std              mean       std  \n",
      "failed_hygiene                                         \n",
      "False           11.058265          3.714102  0.781775  \n",
      "True            14.886718          3.465266  0.803280  \n"
     ]
    }
   ],
   "source": [
    "# Sanity check on training versus testing split\n",
    "print(\"Testing Data Statistics\\n-----\")\n",
    "print(dfTest.groupby([\"failed_hygiene\"]).agg({\n",
    "    \"review_text\": [\"count\"],\n",
    "    \"review_text_len\": [\"mean\", \"std\"],\n",
    "    \"review_count\": [\"mean\", \"std\"],\n",
    "    \"review_score_mean\": [\"mean\", \"std\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Append mean restaurant score and number of reviews to LSA\n",
    "# training and testing data\n",
    "trainTermsLSA_Plus = np.append(trainTermsLSA, \\\n",
    "        dfTrain.loc[:, [\"review_count\", \"review_score_mean\"]].values, axis=1)\n",
    "testTermsLSA_Plus = np.append(testTermsLSA, \\\n",
    "        dfTest.loc[:, [\"review_count\", \"review_score_mean\"]].values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression on LSA Plus Restaurant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create logistic regression model from LSA with\n",
    "# mean restaurant score and number of reviews\n",
    "model_LSA_Plus_LR = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.8 ms, sys: 0 ns, total: 93.8 ms\n",
      "Wall time: 64.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train logistic regression model on LSA and mean\n",
    "# restaurant score and number of reviews\n",
    "model_LSA_Plus_LR = model_LSA_Plus_LR.fit(\\\n",
    "        trainTermsLSA_Plus, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 05: Append Restaurant Features to LSA of Unigram Frequency\n",
      "-----\n",
      "F-1 Score: 0.674419\n",
      "True Negatives: 50\n",
      "True Positives: 58\n",
      "False Negatives: 28\n",
      "False Positives: 28\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_LSA_Plus_LR_Pred = model_LSA_Plus_LR.predict(testTermsLSA_Plus)\n",
    "printModelF1(dfTest.failed_hygiene, model_LSA_Plus_LR_Pred, \\\n",
    "             \"Model 05: Append Restaurant Features to LSA of Unigram Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the restaurant review count and mean score produces **identical** results to the unigram LSA into logistic regression model (#03). That result surprised me. I anticipated the review count and mean score would improve the predictive quality of logistic regression. Two explanations come to mind for breaking that expectation:\n",
    "\n",
    "* Most reviews do not emphasize hygiene *even* in their scores. A separate analysis on the individual reviews might dis/prove that hypothesis. As noted previously, this training data concatenates all reviews and provides only mean score.\n",
    "* The scores and reviews lack proximity (in time) to failed hygiene inspections. A restaurant might have failed its hygiene inspection only days before compiling the training data just as easily as another restaurant failed an inspection years prior. Their concatenated reviews and mean scores would not capture the timing discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Additional Algorithms\n",
    "\n",
    "The previous models all employed logistic regression on different representation of review text and restaurant features. Model #03 using LSA on unigram frequencies proved the most predictive with an F1 of 0.67. I do not anticipate significantly better performance from other algorithms; concatenated review text makes a weak a predictor of failed hygiene inspection. Yet [SciKit Learn](http://scikit-learn.org/stable/index.html) standardizes the training and prediction for many different algorithms including K-Nearest Neighbors, Random Forests, and Naïve Bayes. I test several below. None improve on model #03 enough to warrant their additional complexity and processing times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 06: K-Nearest Neighbor Classifier of Unigram Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create KNN classifier\n",
    "model_KNN = KNeighborsClassifier(n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 438 ms, sys: 0 ns, total: 438 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train KNN classifier on training terms\n",
    "model_KNN = model_KNN.fit(trainP, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 06: K-Nearest Neighbor Classifier of Unigram Probability\n",
      "-----\n",
      "F-1 Score: 0.521212\n",
      "True Negatives: 42\n",
      "True Positives: 43\n",
      "False Negatives: 43\n",
      "False Positives: 36\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_KNN_Pred = model_KNN.predict(testP)\n",
    "printModelF1(dfTest.failed_hygiene, model_KNN_Pred, \\\n",
    "             \"Model 06: K-Nearest Neighbor Classifier of Unigram Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 07: Random Forest Classifier of Unigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create Random Forest classifier\n",
    "model_RF = RandomForestClassifier(n_estimators=55, criterion=\"entropy\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 0 ns, total: 188 ms\n",
      "Wall time: 207 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train Random Forest classifier on training terms\n",
    "model_RF = model_RF.fit(trainTerms, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 07: Random Forest Classifier of Unigram Frequency\n",
      "-----\n",
      "F-1 Score: 0.627219\n",
      "True Negatives: 48\n",
      "True Positives: 53\n",
      "False Negatives: 33\n",
      "False Positives: 30\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_RF_Pred = model_RF.predict(testTerms)\n",
    "printModelF1(dfTest.failed_hygiene, model_RF_Pred, \\\n",
    "             \"Model 07: Random Forest Classifier of Unigram Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 08: Naïve Bayes of Binary Unigram Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create binary TF vectorizer \n",
    "tb = CountVectorizer(max_df=1.0, min_df=1, \\\n",
    "                     stop_words=\"english\", \\\n",
    "                     tokenizer=MyTokenizer(), \\\n",
    "                     binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 s, sys: 15.6 ms, total: 6.59 s\n",
      "Wall time: 6.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate training term Presence\n",
    "trainBi = tb.fit_transform(dfTrain.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 s, sys: 0 ns, total: 2.73 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate testing term Presence; Note: Transform ONLY,\n",
    "# no additional fitting\n",
    "testBi = tb.transform(dfTest.review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create Bernoulli Naïve Bayes classifier\n",
    "model_NB = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train Bernoulli Naïve Bayes classifier on training term\n",
    "# presences\n",
    "model_NB = model_NB.fit(trainBi, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 08: Naïve Bayes of Binary Unigram Indicator\n",
      "-----\n",
      "F-1 Score: 0.477612\n",
      "True Negatives: 62\n",
      "True Positives: 32\n",
      "False Negatives: 54\n",
      "False Positives: 16\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_NB_Pred = model_NB.predict(testBi)\n",
    "printModelF1(dfTest.failed_hygiene, model_NB_Pred, \\\n",
    "             \"Model 08: Naïve Bayes of Binary Unigram Indicator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 09: Latent Symantic Analysis of Binary Unigram Indicator Before Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create Latent Semantic Analysis instance\n",
    "decomp_LSABi = TruncatedSVD(n_components=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 78.1 ms, total: 1.92 s\n",
      "Wall time: 740 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Perform Latent Semantic Analysis on training term presence\n",
    "decomp_LSABi = decomp_LSABi.fit(trainBi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw training term presence into the\n",
    "# LSA decomposed features\n",
    "trainBiLSA = decomp_LSABi.transform(trainBi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform the raw testing term presence into the\n",
    "# LSA decomposed features\n",
    "testBiLSA = decomp_LSABi.transform(testBi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create Bernoulli Naïve Bayes classifier from\n",
    "# term presence LSA\n",
    "model_NB_LSA = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 0 ns, total: 31.2 ms\n",
      "Wall time: 2.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train Bernoulli Naïve Bayes classifier on training term\n",
    "# presence LSA\n",
    "model_NB_LSA = model_NB_LSA.fit(trainBiLSA, dfTrain.failed_hygiene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 09: Latent Symantic Analysis of Binary Unigram Indicator Before Naïve Bayes\n",
      "-----\n",
      "F-1 Score: 0.617284\n",
      "True Negatives: 52\n",
      "True Positives: 50\n",
      "False Negatives: 36\n",
      "False Positives: 26\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "model_NB_LSA_Pred = model_NB_LSA.predict(testBiLSA)\n",
    "printModelF1(dfTest.failed_hygiene, model_NB_LSA_Pred, \\\n",
    "             \"Model 09: Latent Symantic Analysis of Binary Unigram Indicator Before Naïve Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Best Model 03\n",
    "\n",
    "Predicting hygiene inspection failure using logistic regression of LSA features generated from review unigram frequency proved the best model. It not only yielded the highest F1 score (0.67) but one of the lowest computing overheads in both time and memore. As an added benefit, the steps can execute in parallel by restaurant review (after training the model). Model parameters:\n",
    "\n",
    "* Unigram frequency calculated with [SciKit Learn `CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "    * Applies custom tokenizer to lemmatize and stem tokens using [NLTK `WordNetLemmatizer`](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet)\n",
    "    * Counts all tokens (i.e., minimum document frequency of 1 and maximum of 100%)\n",
    "    * Removes English stop words\n",
    "* Latent Symantic Analysis of unigram frequency with [SciKit Learn `TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
    "    * Decomposed into 180 components\n",
    "    * Default randomized algorithm due to Halko (2009)\n",
    "* Logistic Regression of LSA features with [SciKit Learn `LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "    * Default to L2 penalty\n",
    "    * No class weighting (i.e., assume pass/fail hygiene inspection have equal probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPhrase Publications\n",
    "\n",
    "[AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) arose form the contributions of two publications:\n",
    "\n",
    "1. Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, \"[Automated Phrase Mining from Massive Text Corpora](https://arxiv.org/abs/1702.04457)\", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.\n",
    "2. Jialu Liu*, Jingbo Shang*, Chi Wang, Xiang Ren and Jiawei Han, \"[Mining Quality Phrases from Massive Text Corpora](http://hanj.cs.illinois.edu/pdf/sigmod15_jliu.pdf)”, Proc. of 2015 ACM SIGMOD Int. Conf. on Management of Data (SIGMOD'15), Melbourne, Australia, May 2015. (* equally contributed, [slides](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/sigmod15SegPhrase.pdf))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
